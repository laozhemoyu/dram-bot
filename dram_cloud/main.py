# main.py (GitHub äº‘ç«¯ä¸“ç”¨ç‰ˆ)
import os
import time
import hmac
import hashlib
import base64
import urllib.parse
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options # ğŸ‘ˆ æ”¹æˆäº† Chrome
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# ==========================================
# ğŸ”‘ ä» GitHub ç¯å¢ƒå˜é‡è¯»å–é…ç½®
# ==========================================
WEBHOOK = os.environ.get("DING_WEBHOOK")
SECRET = os.environ.get("DING_SECRET")

def send_dingtalk_markdown(title, content):
    """å‘é€ Markdown æ¶ˆæ¯"""
    if not WEBHOOK or not SECRET:
        print("âŒ é”™è¯¯: æœªè¯»å–åˆ°é’‰é’‰é…ç½®ï¼Œè¯·æ£€æŸ¥ GitHub Secretsï¼")
        return

    timestamp = str(round(time.time() * 1000))
    secret_enc = SECRET.encode('utf-8')
    string_to_sign = '{}\n{}'.format(timestamp, SECRET)
    string_to_sign_enc = string_to_sign.encode('utf-8')
    hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    
    url = f"{WEBHOOK}&timestamp={timestamp}&sign={sign}"
    headers = {'Content-Type': 'application/json'}
    data = {"msgtype": "markdown", "markdown": {"title": title, "text": content}}
    
    try:
        resp = requests.post(url, headers=headers, json=data, timeout=10)
        print(f"ğŸ“¨ æ¨é€å“åº”: {resp.status_code}")
    except Exception as e:
        print(f"âŒ æ¨é€å¤±è´¥: {e}")

def generate_report(data_list):
    """ç”Ÿæˆæ¼‚äº®çš„å¡ç‰‡æŠ¥å‘Š"""
    if not data_list: return "æš‚æ— æ•°æ®"
    
    parsed = []
    for item in data_list:
        try:
            parts = item.split("|")
            val_str = parts[2].replace("æ¶¨è·Œ:", "").replace("%", "").strip()
            val = float(val_str) if val_str not in ["", "-"] else 0
            parsed.append({"raw": item, "val": val})
        except: continue

    # æ’åºï¼šæ¶¨åœ¨å‰ï¼Œè·Œåœ¨å
    up = sorted([x for x in parsed if x['val'] > 0], key=lambda x: x['val'], reverse=True)
    down = sorted([x for x in parsed if x['val'] < 0], key=lambda x: x['val'])
    flat = [x for x in parsed if x['val'] == 0]

    lines = [f"## ğŸ“Š DRAM è¡Œæƒ… (GitHubç‰ˆ)", f"> æ—¶é—´: {time.strftime('%H:%M')}", "---"]
    
    def add_section(items, title, icon):
        if items:
            lines.append(f"### {icon} {title} ({len(items)})")
            for item in items:
                parts = item['raw'].split("|")
                name = parts[0].strip()
                price = parts[1].replace("å‡ä»·:", "").strip()
                change = parts[2].replace("æ¶¨è·Œ:", "").strip()
                lines.append(f"**{name}**\n- ğŸ’° `{price}` ({change})\n")

    add_section(up, "é¢†æ¶¨", "ğŸ”´")
    add_section(down, "é¢†è·Œ", "ğŸ’š")
    
    if flat:
        lines.append(f"### â– æŒå¹³ ({len(flat)})")
        for x in flat:
            parts = x['raw'].split("|")
            lines.append(f"- {parts[0].strip()}")
            
    return "\n".join(lines)

def scrape_data():
    """Chrome çˆ¬è™«å¼•æ“ (é€‚é… GitHub Linux ç¯å¢ƒ)"""
    print("ğŸŒ æ­£åœ¨å¯åŠ¨ Chrome...")
    options = Options()
    options.add_argument("--headless=new") # æ— å¤´æ¨¡å¼
    options.add_argument("--no-sandbox")   # Linux å¿…é¡»å‚æ•°
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    # ä¼ªè£… User-Agent
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    # âŒ æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦æŒ‡å®š executable_pathï¼ŒGitHub ä¼šè‡ªåŠ¨å¤„ç†
    driver = webdriver.Chrome(options=options)
    
    try:
        print("â¡ï¸ è®¿é—® TrendForce...")
        driver.get("https://www.trendforce.cn/price")
        time.sleep(5)
        
        # å°è¯•ç‚¹å‡» DRAM
        try:
            print("ğŸ–±ï¸ å°è¯•ç‚¹å‡»æŒ‰é’®...")
            btn = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, "//*[contains(text(), 'DRAM')]")))
            driver.execute_script("arguments[0].click();", btn)
            time.sleep(3)
        except: 
            print("âš ï¸ æŒ‰é’®ç‚¹å‡»è·³è¿‡")
        
        print("â³ è§£ææ•°æ®...")
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        res = []
        for row in soup.select('table tbody tr') or soup.select('table tr'):
            cols = row.find_all(['th', 'td'])
            if len(cols) < 7: continue
            name = cols[0].get_text(strip=True)
            if 'DDR' in name.upper():
                try:
                    p = cols[5].get_text(strip=True)
                    c = cols[6].get_text(strip=True)
                    res.append(f"{name} | å‡ä»·:{p} | æ¶¨è·Œ:{c}")
                except: continue
        return res
    except Exception as e:
        print(f"âŒ é”™è¯¯è¯¦æƒ…: {e}")
        return []
    finally:
        driver.quit()

if __name__ == "__main__":
    print("ğŸš€ è„šæœ¬å¼€å§‹è¿è¡Œ...")
    data = scrape_data()
    if data:
        print(f"âœ… æŠ“å–æˆåŠŸ: {len(data)} æ¡")
        report = generate_report(data)
        send_dingtalk_markdown("DRAMæ—¥æŠ¥", report)
    else:
        print("âŒ æœªæŠ“å–åˆ°æ•°æ®")
