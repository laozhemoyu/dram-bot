# main.py
import os
import time
import hmac
import hashlib
import base64
import urllib.parse
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# ==========================================
# ğŸ”‘ ä» GitHub ç¯å¢ƒå˜é‡è¯»å–é…ç½® (å®‰å…¨ï¼)
# ==========================================
# åé¢ä¼šåœ¨ GitHub ç½‘é¡µä¸Šè®¾ç½®è¿™ä¸¤ä¸ªå˜é‡
WEBHOOK = os.environ.get("DING_WEBHOOK")
SECRET = os.environ.get("DING_SECRET")

def send_dingtalk_markdown(title, content):
    """å‘é€æ¶ˆæ¯åˆ°é’‰é’‰"""
    if not WEBHOOK or not SECRET:
        print("âŒ é”™è¯¯: æœªæ£€æµ‹åˆ°é’‰é’‰é…ç½®ï¼Œè¯·åœ¨ GitHub Secrets ä¸­è®¾ç½®ï¼")
        return

    timestamp = str(round(time.time() * 1000))
    secret_enc = SECRET.encode('utf-8')
    string_to_sign = '{}\n{}'.format(timestamp, SECRET)
    string_to_sign_enc = string_to_sign.encode('utf-8')
    hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest()
    sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))
    
    url = f"{WEBHOOK}&timestamp={timestamp}&sign={sign}"
    headers = {'Content-Type': 'application/json'}
    data = {"msgtype": "markdown", "markdown": {"title": title, "text": content}}
    
    try:
        requests.post(url, headers=headers, json=data, timeout=10)
        print("âœ… æ¨é€æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨é€å¤±è´¥: {e}")

def generate_report(data_list):
    """ç”Ÿæˆæ¼‚äº®çš„å¡ç‰‡æŠ¥å‘Š"""
    if not data_list: return "æš‚æ— æ•°æ®"
    
    # è§£ææ•°æ®
    parsed = []
    for item in data_list:
        try:
            parts = item.split("|")
            # æå–æ¶¨è·Œæ•°å€¼ç”¨äºæ’åº
            val_str = parts[2].replace("æ¶¨è·Œ:", "").replace("%", "").strip()
            val = float(val_str) if val_str not in ["", "-"] else 0
            parsed.append({"raw": item, "val": val})
        except: continue

    # æ’åºï¼šæ¶¨çš„åœ¨å‰ï¼Œè·Œçš„åœ¨å
    up = sorted([x for x in parsed if x['val'] > 0], key=lambda x: x['val'], reverse=True)
    down = sorted([x for x in parsed if x['val'] < 0], key=lambda x: x['val'])
    flat = [x for x in parsed if x['val'] == 0]

    lines = [f"## ğŸ“Š DRAM è¡Œæƒ… (GitHubäº‘ç«¯)", f"> æ›´æ–°: {time.strftime('%H:%M')}", "---"]
    
    # è¾…åŠ©æ˜¾ç¤ºå‡½æ•°
    def add_section(items, title, icon):
        if items:
            lines.append(f"### {icon} {title} ({len(items)})")
            for item in items:
                parts = item['raw'].split("|")
                name = parts[0].strip()
                price = parts[1].replace("å‡ä»·:", "").strip()
                change = parts[2].replace("æ¶¨è·Œ:", "").strip()
                lines.append(f"**{name}**\n- ğŸ’° `{price}` ({change})\n")

    add_section(up, "é¢†æ¶¨", "ğŸ”´")
    add_section(down, "é¢†è·Œ", "ğŸ’š")
    
    if flat:
        lines.append(f"### â– æŒå¹³ ({len(flat)})")
        for x in flat[:10]: # åªæ˜¾ç¤ºå‰10ä¸ªé˜²æ­¢å¤ªé•¿
            parts = x['raw'].split("|")
            lines.append(f"- {parts[0].strip()}")
            
    return "\n".join(lines)

def scrape_data():
    """Chrome çˆ¬è™«å¼•æ“"""
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    # GitHub Actions è‡ªå¸¦ Chrome å’Œ Driverï¼Œæ— éœ€æŒ‡å®šè·¯å¾„
    driver = webdriver.Chrome(options=options)
    
    try:
        print("ğŸŒ è®¿é—® TrendForce...")
        driver.get("https://www.trendforce.cn/price")
        time.sleep(5)
        
        # å°è¯•ç‚¹å‡» DRAM
        try:
            btn = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, "//*[contains(text(), 'DRAM')]")))
            driver.execute_script("arguments[0].click();", btn)
            time.sleep(3)
        except: pass
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        res = []
        for row in soup.select('table tbody tr') or soup.select('table tr'):
            cols = row.find_all(['th', 'td'])
            if len(cols) < 7: continue
            name = cols[0].get_text(strip=True)
            if 'DDR' in name.upper():
                try:
                    p = cols[5].get_text(strip=True)
                    c = cols[6].get_text(strip=True)
                    res.append(f"{name} | å‡ä»·:{p} | æ¶¨è·Œ:{c}")
                except: continue
        return res
    except Exception as e:
        print(f"Error: {e}")
        return []
    finally:
        driver.quit()

if __name__ == "__main__":
    print("ğŸš€ äº‘ç«¯çˆ¬è™«å¯åŠ¨...")
    data = scrape_data()
    if data:
        print(f"âœ… æŠ“å–åˆ° {len(data)} æ¡æ•°æ®")
        report = generate_report(data)
        send_dingtalk_markdown("DRAMæ—¥æŠ¥", report)
    else:
        print("âŒ æœªæŠ“å–åˆ°æ•°æ®")